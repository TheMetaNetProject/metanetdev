"""
.. module:: metanetdb
   :synopsis: ORM-based module (via peewee) for interacting with MetaNet LM example database
   
Contains classes autogenerated via peewee that mirror the structure of the database, as well
as higher-level routines for inserting LMs into the database.

.. moduleauthor:: Jisup <jhong@icsi.berkeley.edu>
"""

from peewee import *
from playhouse.proxy import Proxy
import json, re, sys, logging, pprint, traceback, os, codecs, setproctitle
import MySQLdb
from metanetrdf import MetaNetRepository
from mnformats import mnjson
import argparse
from multiprocessing import Pool

mnlmdatabase_proxy = Proxy()

class UnknownFieldType(object):
    pass

class BaseModel(Model):
    class Meta:
        database = mnlmdatabase_proxy

class Lm(BaseModel):
    cm = CharField(null=True)
    cxn = CharField(null=True)
    name = CharField(null=True)
    nickname = CharField(null=True)
    sourcelemma = CharField(null=True)
    sourceframe = CharField(null=True)
    sourceframefamily = CharField(null=True)
    sourceconcept = CharField(null=True)
    sourcemapping = CharField(null=True)
    status = CharField(null=True)
    targetlemma = CharField(null=True)
    targetframe = CharField(null=True)
    targetconcept = CharField(null=True)
    lang = CharField()
    score = FloatField(null=True)
    promrcs = CharField(null=True)
    conmrcs = CharField(null=True)
    
    class Meta:
        db_table = 'LM'

class Document(BaseModel):
    uref = CharField()
    corpus = CharField(null=True)
    description = CharField(null=True)
    lang = CharField()
    name = CharField(null=True)
    size = IntegerField(null=True)
    type = CharField(null=True)
    perspective = CharField(null=True)
    uri = TextField(null=True)

    class Meta:
        db_table = 'document'

class Sentence(BaseModel):
    document = ForeignKeyField(db_column='document_id', rel_model=Document)
    ext = CharField(db_column='ext_id')
    text = TextField()

    class Meta:
        db_table = 'sentence'

class Lm_Instance(BaseModel):
    lm = ForeignKeyField(db_column='LM_id', rel_model=Lm)
    classifier = CharField(null=True)
    comments = CharField(null=True)
    data = TextField(null=True)
    extractor = CharField(null=True)
    sentence = ForeignKeyField(db_column='sentence_id', rel_model=Sentence)
    sourcespans = CharField(null=True)
    status = CharField(null=True)
    tags = CharField(null=True)
    targetspans = CharField(null=True)

    class Meta:
        db_table = 'LM_instance'

class Gmr_Mapping(BaseModel):
    sourceframe = CharField(null=True)
    sourceconcept = CharField(null=True)
    
    class Meta:
        db_table = 'GMR_mapping'

class MetaNetLMDB:
    """ Class for accessing (query or generate) the MetaNet LM database.
    """
    def __init__(self, host=None, socket=None, user=None, passwd=None, dbname='metanetlm'):
        """
        :param host: database host name (currently unused)
        :type host: str
        :param socket: database socket
        :type socket: str
        :param user: database user name
        :type user: str
        :param passwd: database password
        :type passwd: str
        :param dbname: name of database
        :type dbname: str
        """
        global mnlmdatabase_proxy
        mydb = MySQLDatabase(dbname, **{'user':user,
                                        'passwd': passwd,
                                        'unix_socket': socket,
                                        'charset':'utf8',
                                        'use_unicode':True})
        mnlmdatabase_proxy.initialize(mydb)
        mnlmdatabase_proxy.connect()
        self.logger = logging.getLogger(__name__)
        self.lmcache = {}
    
    def update_document(self,lang,uref,name=None,corpus=None,desc=None,typeval=None,perspective=None,uri=None,size=None):
        """ Documents are uniquely identified by URI.  Retrieve or create,
            and then update fields if values are given.
        """
        if self.fromEmpty:
            document = Document()
            document.uref=uref
            document.lang=lang
        else:
            try:
                document = Document.get(Document.uref==uref)
            except DoesNotExist:
                document = Document()
                document.uref=uref
                document.lang=lang
        if corpus:
            document.corpus = corpus
        if name:
            document.name = name
        if desc:
            document.description = desc
        if typeval:
            document.type = typeval
        if perspective:
            document.perspective = perspective
        if uri:
            document.uri = uri
        if size > 0:
            document.size = size
        try:
            document.save()
        except:
            self.logger.error(u'Error saving document %s, %s, %s, %s, %s, %d',
                              corpus,desc,uri,lang,typeval,size)
            raise
        return document
    
    def create_sentence(self,extid,text,doc):
        """ Sentences are uniquely identified by external id.  Retrieve
            or create.  Sentences should not be updated once they are
            created.
        """
        if self.fromEmpty:
            sentence = Sentence()
            sentence.ext=extid
            sentence.text=text
            sentence.document=doc
        else:
            try:
                sentence = Sentence.get(Sentence.ext==extid)
                return sentence
            except DoesNotExist:
                sentence = Sentence()
                sentence.ext=extid
                sentence.text=text
                sentence.document=doc
        try:
            sentence.save()
        except:
            self.logger.error(u'Error saving sentence %s: %s',extid,text)
            raise
        return sentence
    
    def getname(self, lm):
        """ Given an LM item, generate a unique name. Should be case
        insensitive because that's now MySQL is. """
        try:
            if ('target' not in lm) or ('source' not in lm):
                return None
            if ('lemma' not in lm['target']) or ('lemma' not in lm['source']):
                return None
            if 'cxn' in lm:
                name = u'%s(T=%s,S=%s)' % (lm['cxn'],lm['target']['lemma'],
                                           lm['source']['lemma'])
            elif ('rel' in lm['target']) and ('rel' in lm['source']):
                name = u'%s-%s(T=%s,S=%s)' % (lm['target']['rel'],
                                              lm['source']['rel'],
                                              lm['target']['lemma'],
                                              lm['source']['lemma'])
            else:
                name = u'unknown(T=%s,S=%s)' % (lm['target']['lemma'],
                                                lm['source']['lemma'])
        except:
            self.logger.error(u'Error generating name of LM')
            raise
        return name.lower()
    
    def update_lm(self,lang, lmname, lm):
        """ LMs are uniquely identified by name.  Name should consist
            of predictable amalgam of target, source, and cxn.
            The nickname field is used to store a pretty name for
            display purposes
        """
        if lmname in self.lmcache:
            lingmet = self.lmcache[lmname]
        else:
            try:
                lingmet = Lm.get(Lm.name==lmname)
            except DoesNotExist:
                lingmet = Lm()
                lingmet.name = lmname
                lingmet.nickname = lm['name']
                lingmet.sourcelemma = lm['source']['lemma']
                lingmet.targetlemma = lm['target']['lemma']
                lingmet.lang = lang
            self.lmcache[lmname] = lingmet
        
        # cxn, frame, etc. can be updated
        if 'cxn' in lm:
            if (lingmet.cxn) and (lingmet.cxn != lm['cxn']):
                self.logger.debug(u'Updating cxn for lm (%s): %s -> %s',
                                 lmname,lingmet.cxn,lm['cxn'])
            lingmet.cxn = lm['cxn']
        if 'frame' in lm['source']:
            if (lingmet.sourceframe) and (lingmet.sourceframe != lm['source']['frame']):
                self.logger.debug(u'Updating source frame for lm (%s): %s -> %s',
                                 lmname,lingmet.sourceframe,lm['source']['frame'])
            lingmet.sourceframe = lm['source']['frame']
        if 'frame' in lm['target']:
            if (lingmet.targetframe) and (lingmet.targetframe != lm['target']['frame']):
                self.logger.debug(u'Updating target frame for lm (%s): %s -> %s',
                                 lmname,lingmet.targetframe,lm['target']['frame'])
            lingmet.targetframe = lm['target']['frame']
        if 'cm' in lm:
            if (lingmet.cm) and (lingmet.cm != lm['cm']):
                self.logger.debug(u'Updating cm for lm (%s): %s -> %s',
                                 lmname,lingmet.cm,lm['cm'])
            lingmet.cm = lm['cm']
        if 'concept' in lm['target']:
            lingmet.targetconcept = lm['target']['concept']
        if ('concept' in lm['source']) and (lm['source']['concept'] != 'NULL'):
            lingmet.sourceconcept = lm['source']['concept']
        if 'score' in lm:
            lingmet.score = lm['score']
        lingmet.save()
        return lingmet

    def getLMs(self,lang=None,naive=True):
        """ Returns an iterator over all LMs """
        if lang:
            if naive:
                lmq = Lm.select().where(Lm.lang==lang).naive()
            else:
                return Lm.select().where(Lm.lang==lang)
        else:
            if naive:
                lmq = Lm.select().naive()
            else:
                return Lm.select()
        lmq.execute()
        return lmq.iterator()
    
    def getLMInstances(self,lm,naive=True):
        """ Returns an iterator of all LM instances """
        if naive:
            lmq = Lm_Instance.select().where(Lm_Instance.lm==lm).naive()
            lmq.execute()
            return lmq.iterator()
        else:
            return Lm_Instance.select().where(Lm_Instance.lm==lm)
    
    def getLMFromInstanceID(self,lmiid):
        return (Lm_Instance.select(Lm_Instance.id,Lm.id,
            Lm.targetlemma,Lm.targetframe,
            Lm.sourcelemma,Lm.sourceframe,Lm.sourceframefamily,Lm.sourceconcept,
            Lm.cxn,Lm.sourcemapping,
            Lm.score,Lm.cm,
            Document.corpus,Document.name,Document.type,
            Document.perspective,Sentence.id)
            .join(Lm).switch(Lm_Instance).join(Sentence).join(Document)
            .where(Lm_Instance.id==lmiid))
    
    def getUrefFromSID(self,sid):
        """ Remove final ':num' from sid """
        return re.sub(ur':[0-9_]+$',u'',sid,count=1,flags=re.U)
    
    def getDocFromUref(self,uref):
        try:
            document = Document.get(Document.uref==uref)
        except DoesNotExist:
            self.logger.error("Couldn't find Document %s",uref)
            raise
        return document
    
    def getLMsWithTargetConcept(self,tcon):
        return Lm.select().where(Lm.targetconcept==tcon) 
    
    def getCurrentId(self, tablename, lang):
        if tablename == 'document':
            return Document.select(fn.Max(Document.id)).where(Document.lang==lang).scalar()
        if tablename == 'sentence':
            return Sentence.select(fn.Max(Sentence.id)).join(Document).where(Document.lang==lang).scalar()
        if tablename == 'LM':
            return Lm.select(fn.Max(Lm.id)).where(Lm.lang==lang).scalar()
        if tablename == 'LM_instance':
            return Lm_Instance.select(fn.Max(Lm_Instance.id)).join(Lm).where(Lm.lang==lang).scalar()
        
    def getRefHash(self, tablename, lang):
        rhash = {}
        if tablename=='document':
            for row in Document.select(Document.id,Document.uref).where(Document.lang==lang):
                rhash[row.uref] = row.id
            return rhash
        if tablename=='sentence':
            for row in Sentence.select(Sentence.id,Sentence.ext).join(Document).where(Document.lang==lang):
                rhash[row.ext] = row.id
            return rhash
        if tablename=='LM':
            for row in Lm.select(Lm.id,Lm.name).where(Lm.lang==lang):
                rhash[row.name] = row.id
            return rhash
        if tablename=='LM_instance':
            return rhash
        return rhash
        
    def getLMData(self,lang):
        return (Lm_Instance.select(
                    Lm_Instance.id.alias('lmiid'),Lm.id.alias('lmid'),
                    Lm.targetconcept.alias('tconcept'),Lm.targetframe.alias('tframe'),
                    Lm.targetlemma.alias('tlemma'),Lm.cxn.alias('cxn'),
                    Lm.sourcelemma.alias('slemma'),Lm.sourceframe.alias('sframe'),
                    Lm.sourceframefamily.alias('sfamily'),Lm.sourceconcept.alias('sconcept'),
                    Lm.sourcemapping.alias('smapping'),
                    Lm.score.alias('score'),Lm.promrcs.alias('promrcs'),Lm.conmrcs.alias('conmrcs'),
                    Lm.cm.alias('cm'),Document.corpus.alias('corpus'),
                    Document.name.alias('docname'),Document.type.alias('doctype'),
                    Document.perspective.alias('perspective'),
                    Sentence.id.alias('sid'),Sentence.text.alias('text'))
                .join(Lm).switch(Lm_Instance).join(Sentence).join(Document)
                .where((Document.lang==lang))).tuples()
    
    def getLMDataByTargetConceptSubString(self,lang,conceptStr):
        searchString = '%'+conceptStr+'%'
        return (Lm_Instance.select(
                    Lm_Instance.id.alias('lmiid'),Lm.id.alias('lmid'),
                    Lm.targetconcept.alias('tconcept'),Lm.targetframe.alias('tframe'),
                    Lm.targetlemma.alias('tlemma'),Lm.cxn.alias('cxn'),
                    Lm.sourcelemma.alias('slemma'),Lm.sourceframe.alias('sframe'),
                    Lm.sourceframefamily.alias('sfamily'),Lm.sourceconcept.alias('sconcept'),
                    Lm.sourcemapping.alias('smapping'),
                    Lm.score.alias('score'),Lm.promrcs.alias('promrcs'),Lm.conmrcs.alias('conmrcs'),
                    Lm.cm.alias('cm'),Document.corpus.alias('corpus'),
                    Document.name.alias('docname'),Document.type.alias('doctype'),
                    Document.perspective.alias('perspective'),
                    Sentence.id.alias('sid'),Sentence.text.alias('text'))
                .join(Lm).switch(Lm_Instance).join(Sentence).join(Document)
                .where((Document.lang==lang) & (Lm.targetconcept % searchString))).tuples()

    def getNullSourceLMDataByTargetConceptSubString(self,lang,conceptStr):
        """ Returns only LMs that have NULL source concepts.  This is to avoid
        overlap in returned values from the LMs in the GMR.
        """
        searchString = '%'+conceptStr+'%'
        return (Lm_Instance.select(
                    Lm_Instance.id.alias('lmiid'),Lm.id.alias('lmid'),
                    Lm.targetconcept.alias('tconcept'),Lm.targetframe.alias('tframe'),
                    Lm.targetlemma.alias('tlemma'),Lm.cxn.alias('cxn'),
                    Lm.sourcelemma.alias('slemma'),Lm.sourceframe.alias('sframe'),
                    Lm.sourceframefamily.alias('sfamily'),Lm.sourceconcept.alias('sconcept'),
                    Lm.sourcemapping.alias('smapping'),
                    Lm.score.alias('score'),Lm.cm.alias('cm'),Document.corpus.alias('corpus'),
                    Document.name.alias('docname'),Document.type.alias('doctype'),
                    Document.perspective.alias('perspective'),
                    Sentence.id.alias('sid'),Sentence.text.alias('text'))
                .join(Lm).switch(Lm_Instance).join(Sentence).join(Document)
                .where((Document.lang==lang) & (Lm.targetconcept % searchString) & 
                    (Lm.sourceconcept >> None) & ~(Lm.sourceframe >> None))).tuples()
    
    def importLMs(self,jdata, fromEmpty=False, conceptOnly=False, docByName=False, urefField='name'):
        """ Import LMs into the database from the JSON document structure.
        :param jdata: JSON dict containing doc metadata and sentences
        :type jdata: dict
        """
        lang = jdata['lang']
        docheaders = []
        cndupere = re.compile('(:CNMS)+')
        
        self.fromEmpty = fromEmpty
        
        self.logger.info(' processing document headers')
        if 'document' in jdata:
            docheaders.append(jdata['document'])
        else:
            docheaders = jdata['documents']
        for docheader in docheaders:
            try:
                if 'corpus' not in docheader:
                    docheader['corpus'] = None
                if 'description' not in docheader:
                    docheader['description'] = None
                if 'type' not in docheader:
                    docheader['type'] = None
                if 'provenance' not in docheader:
                    docheader['provenance'] = None
                if 'size' not in docheader:
                    docheader['size'] = None
                #print docheader
                document = self.update_document(lang,
                                                docheader[urefField],
                                                docheader['name'],
                                                docheader['corpus'],
                                                docheader['description'],
                                                docheader['type'],
                                                docheader['provenance'],
                                                docheader['size'])
            except:
                print >> sys.stderr, "Error with docheader,",docheader
                raise
            
        self.logger.info(' processing sentences')
        docuref = None
        document = None
        numsents = 0
        numlmi = 0
        for sent in jdata['sentences']:
            # flag to use to import sentence only if necessary
            sentenceImported = False
            # skip sentence if no lms
            if (u'lms' not in sent) or (len(sent[u'lms'])==0):
                continue
            # used to skip duplicate LM instances
            lmicache = set()
            try:
                for lm in sent['lms']:
                    # sanity check, if LM name is superlong, skip it
                    if len(lm['name']) > 255:
                        continue
                    lmname = self.getname(lm)
                    if not lmname:
                        # no name? skip
                        continue
                    try:
                        spans = '%d-%d' % (lm['source']['start'],lm['source']['end'])
                        spant = '%d-%d' % (lm['target']['start'],lm['target']['end'])
                        spankey = spans + spant
                    except TypeError:
                        # skip that LM because it has invalid start/end values
                        continue
                    if spankey in lmicache:
                        # the same span of the sentence already has an LM defined on it
                        continue
                    
                    if conceptOnly:
                        # if conceptOnly flag is set, skip LMs with no concept mapping
                        if 'concept' not in lm['target']:
                            continue
                        if ('concept' not in lm['source']) or (lm['source']['concept'] == 'NULL'):
                            continue

                    # insert LM into DB
                    try:
                        lingmet = self.update_lm(lang, lmname, lm)
                    except:
                        print >> sys.stderr, "Error saving lm to database"
                        raise
                    
                    # Now we know we there's an LM instance here, so insert the sentence
                    if not sentenceImported:
                        ndocuref = self.getUrefFromSID(sent[u'id'])
                        if ndocuref != docuref:
                            docuref = ndocuref
                            document = self.getDocFromUref(docuref)
                        try:
                            sentence = self.create_sentence(sent[u'id'], sent[u'text'], document)
                        except MySQLdb.OperationalError as oe:
                            if 'Incorrect string value' in str(oe):
                                self.logger.error('Skipping sentence %s because of error: %s',
                                                  sent[u'id'],str(oe))
                            continue
                        except:
                            print >> sys.stderr, "Error saving sentence to DB."
                            raise
                        numsents += 1
                        sentenceImported = True
                    
                    if not self.fromEmpty:
                        # if it's not starting from empty, there's a chance that the LMI
                        # could have been inserted from a previous iteration
                        try:
                            lmi = Lm_Instance.get(Lm_Instance.lm==lingmet,
                                                  Lm_Instance.sentence==sentence,
                                                  Lm_Instance.sourcespans==spans,
                                                  Lm_Instance.targetspans==spant)
                            # if found, then cache, and skip to next
                            lmicache.add(spankey)
                            continue
                        except DoesNotExist:
                            pass
                    numlmi += 1
                    lmi = Lm_Instance()
                    lmi.lm = lingmet
                    lmi.sentence = sentence
                    lmi.targetspans = spant
                    lmi.sourcespans = spans
                    if 'extractor' in lm:
                        lmi.extractor = cndupere.sub(':CNMS', lm['extractor'])
                    elif 'cxn' in lm:
                        lmi.extractor = 'CMS'
                        data = {}
                        if 'score' in lm:
                            data['score'] = lm['score']
                        if 'mword' in lm['source']:
                            data['source mword'] = lm['source']['mword']
                        if 'mword' in lm['target']:
                            data['target mword'] = lm['target']['mword']
                        lmi.data = json.dumps(data)
                    elif ('seed' in lm):
                        if lm['seed'] == 'NA':
                            # Persian
                            lmi.extractor = 'LMS'
                        elif lm['seed'] != 'None':
                            lmi.extractor = 'SBS'
                            data = {'seed':lm['seed']}
                            lmi.data = json.dumps(data)
                    
                    try:
                        lmi.save()
                    except:
                        self.logger.error("Error saving LM_i to database:\n%s\n%s",
                                          pprint.pformat(lmi),
                                          traceback.format_exc())
                        raise
                    lmicache.add(spankey) 
            except:
                self.logger.error(u'Error processing sentence:\n%s\n%s',
                                  pprint.pformat(sent),
                                  traceback.format_exc())
                raise
        self.logger.info("imported %d sentences and %d LM instances",numsents, numlmi)

    def updateLMFrameFields(self, lang):
        """
        Iterates through the LMs in the database and fills in
        the targetframe and sourceframe fields based on
        the targetlemma and sourcelemma fields
        """
        mr = MetaNetRepository(lang)
        mr.initLookups()
        for lm in self.getLMs(lang):
            # for each lm target / source word
            # try to connect it to some frame
            # TARGET SCHEMAS
            doSave = False
            tframes = mr.lookupFramesFromLU(lm.targetlemma)
            tsNameList = []
            for tframe in tframes:
                tsName = mr.getNameString(tframe)
                tsNameList.append(tsName)
            if tsNameList:
                lm.targetframe = u','.join(tsNameList)
                doSave = True
            # SOURCE SCHEMAS
            sframes = mr.lookupFramesFromLU(lm.sourcelemma)
            ssNameList = []
            for sframe in sframes:
                ssName = mr.getNameString(sframe)
                ssNameList.append(ssName)
            if ssNameList:
                lm.sourceframe = u','.join(ssNameList)
                doSave = True
            if doSave:
                lm.save()
    
    def loadData(self,filename,table,lock=True):
        global mnlmdatabase_proxy
        querybuf = []
        if lock:
            querybuf.append('LOCK TABLES `%s` WRITE;' % (table))
        # SJD was LOAD DATA LOCAL INFILE; local dropped to give more diagnostics
        querybuf.append('LOAD DATA LOCAL INFILE \'%s\' INTO TABLE `%s` CHARACTER SET utf8 FIELDS TERMINATED BY \',\' OPTIONALLY ENCLOSED BY \'"\' LINES TERMINATED BY \'\\n\';' % (filename,table))
        if lock:
            querybuf.append('UNLOCK TABLES;')
        #qstring = ''.join(querybuf)
        for qstring in querybuf:
            try:
                mnlmdatabase_proxy.execute_sql(qstring)
            except:
                self.logger.error("Failed on query %s", qstring)
                raise
            
    def deleteData(self,table,idstart,idend):
        """ Delete rows from the table whose IDs are inclusively within
        the range of IDs given.
        """
        delquery = 'DELETE FROM %s WHERE id >= %d AND id <= %d;' % (table, idstart, idend)
        try:
            mnlmdatabase_proxy.execute_sql(delquery)
        except:
            self.logger.error('Error running delete query %s',delquery)
            raise
        
def main():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        description="Import JSON files containing extracted LMs into MetaNet LM database")
    parser.add_argument('filenamelist',
                        help="Text file containing JSON file(s) to import")
    parser.add_argument('-l','--lang', help="Language of input files.",
                        required=True)
    parser.add_argument('-f','--force',
                        action='store_true',
                        help="Re-import files already imported")
    parser.add_argument('-e','--from-empty',dest='fromempty',action='store_true',
                        help='Assume population from empty DB.')
    parser.add_argument('-u','--mdb-user',dest='mdbuser',default='mdbuser',
                        help='MetaNet LM database username')
    parser.add_argument('-p','--mdb-pw',dest='mdbpw',default=None,required=True,
                        help='MetaNet LM database password')
    parser.add_argument('-n','--mdb-name',dest='mdbname',
                        help='Metanet LM database name. Default: lang + mnlm')
    parser.add_argument('-s','--single',action='store_true',
                        help='Process single JSON file (input file is JSON)')
    parser.add_argument('-v','--verbose',action='store_true',
                        help='Display more status messages')
    parser.add_argument('-c','--concept-only',dest='conceptonly',action='store_true',
                        help='Import LMs only if they map to concepts')
    parser.add_argument('-d','--doc-uref',dest='docureffield', default='name',
                        help='Reference documents by given field.')
    cmdline = parser.parse_args()
    
    # proc title manipulation to hide PW
    pstr = setproctitle.getproctitle()
    pstr = re.sub(ur'(-p|--mdb-pw)(=|\s+)(\S+)',ur'\1\2XXXX',pstr)
    setproctitle.setproctitle(pstr)
    
    # this routine has to write its own files
    msgformat = '%(asctime)-15s - %(message)s'
    dateformat = '%Y-%m-%dT%H:%M:%SZ'
    if cmdline.verbose:
        deflevel = logging.INFO
    else:
        deflevel = logging.WARN
    logging.basicConfig(format=msgformat, datefmt=dateformat, level=deflevel)
        
    if cmdline.mdbname:
        mdbname = cmdline.mdbname
    else:
        mdbname = cmdline.lang + 'mnlm'

    lmdb = MetaNetLMDB(socket='/tmp/mysql.sock',
                       user=cmdline.mdbuser,
                       passwd=cmdline.mdbpw,
                       dbname=mdbname)
    if cmdline.single:
        flist = [cmdline.filenamelist]
    else:
        flist = codecs.open(cmdline.filenamelist,encoding='utf-8')
        
    for line in flist:
        jfile = line.strip()
        (dirname,basename) = os.path.split(jfile)
        if not dirname:
            dirname = '.'
        cmpfname = '%s/.imported.%s.tmp' % (dirname,basename)
        if not cmdline.force:
            if os.path.exists(cmpfname):
                logging.warn('Skipping %s because already imported.',jfile)
                continue
        try:
            logging.info('start importing %s', jfile)
            jdata = mnjson.loadfile(jfile)
            lmdb.importLMs(jdata, cmdline.fromempty, cmdline.conceptonly, cmdline.docbyname, cmdline.docureffield)
            open(cmpfname, 'a').close()
            logging.info('end importing %s', jfile)
        except:
            logging.error(traceback.format_exc())
            logging.error("Error parsing %s",jfile)

if __name__ == "__main__":
    status = main()
    sys.exit(status)

